{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# OBJECTIVE\n",
    "#### To create an apache cassandra database based on the queries given by the analytics team of the startup 'Sparkify' in order to fetch song play data to answer the questions. For that, I first created an ETL pipeline to preprocess the data. After that, with the help of the queries, I created three tables in order to retrieve information to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Preprocessing of the file using ETL pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/workspace\n",
      "['/home/workspace/event_data/2018-11-27-events.csv', '/home/workspace/event_data/2018-11-04-events.csv', '/home/workspace/event_data/2018-11-07-events.csv', '/home/workspace/event_data/2018-11-09-events.csv', '/home/workspace/event_data/2018-11-19-events.csv', '/home/workspace/event_data/2018-11-05-events.csv', '/home/workspace/event_data/2018-11-22-events.csv', '/home/workspace/event_data/2018-11-16-events.csv', '/home/workspace/event_data/2018-11-26-events.csv', '/home/workspace/event_data/2018-11-24-events.csv', '/home/workspace/event_data/2018-11-29-events.csv', '/home/workspace/event_data/2018-11-15-events.csv', '/home/workspace/event_data/2018-11-20-events.csv', '/home/workspace/event_data/2018-11-06-events.csv', '/home/workspace/event_data/2018-11-18-events.csv', '/home/workspace/event_data/2018-11-21-events.csv', '/home/workspace/event_data/2018-11-10-events.csv', '/home/workspace/event_data/2018-11-23-events.csv', '/home/workspace/event_data/2018-11-02-events.csv', '/home/workspace/event_data/2018-11-28-events.csv', '/home/workspace/event_data/2018-11-03-events.csv', '/home/workspace/event_data/2018-11-13-events.csv', '/home/workspace/event_data/2018-11-30-events.csv', '/home/workspace/event_data/2018-11-12-events.csv', '/home/workspace/event_data/2018-11-01-events.csv', '/home/workspace/event_data/2018-11-14-events.csv', '/home/workspace/event_data/2018-11-25-events.csv', '/home/workspace/event_data/2018-11-08-events.csv', '/home/workspace/event_data/2018-11-17-events.csv', '/home/workspace/event_data/2018-11-11-events.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "# Retrieving the path to our event_data.\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# Creating a for loop to create a list of files and collecting each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "# joining the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    print(file_path_list)\n",
    "    print(len(file_path_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Processing the files in file_path_list to create a csv file with fewer columns and non-empty rows that will be use for apache cassandra tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initializing an empty list.\n",
    "full_data_rows_list = [] \n",
    "     \n",
    "for f in file_path_list:\n",
    "\n",
    "# reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile:  \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    " # Storing data line by line in full_data_rows_list.      \n",
    "        for line in csvreader:\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "print(len(full_data_rows_list))\n",
    "\n",
    "print(full_data_rows_list)\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_full csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        # Writing data in event_datafile_new.csv for selected features.  \n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# checking the number of rows in the csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Creating tables as well as inserting and selecting data based on our queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This should make a connection to a Cassandra instance our local machine \n",
    "# (127.0.0.1)\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "\n",
    "# To establish connection and begin executing queries, need a session\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating a Keyspace \n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS project\n",
    "    WITH REPLICATION =\n",
    "    {'class': 'SimpleStrategy', 'replication_factor': 1}\"\"\"\n",
    ")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Setting Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Setting KEYSPACE to the keyspace specified above\n",
    "try:\n",
    "    session.set_keyspace('project')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating queries to ask the following three questions given by the analytical team of Sparkify.\n",
    "\n",
    "#### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "#### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "\n",
    "#### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# i) Query-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Query 1:  Give me the artist, song title and song's length in the music app history that was heard during \\\n",
    "## sessionId = 338, and itemInSession = 4\n",
    "table1= \"CREATE TABLE IF NOT EXISTS song_info_by_session\"\n",
    "\n",
    "# Taking session_id and itemInSession as composite primary keys in order to store \n",
    "# each row in node/s with unique identity.\n",
    "query = table1+'(artist text, song text, length float, sessionId int,\\\n",
    "                 itemInSession int,\\\n",
    "                 PRIMARY KEY(sessionId,itemInSession))'\n",
    "try:\n",
    "    session.execute(query)\n",
    "    \n",
    "except Exception as e:\n",
    "    \n",
    "    print(e)        \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "#Reading the event_datafile_new.csv\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "## Assigning the INSERT statements into the `query` variable\n",
    "        query = \"INSERT INTO song_info_by_session (artist, song, length, sessionId, itemInSession)\"\n",
    "        query = query + \"VALUES (%s, %s, %s, %s, %s)\"\n",
    "        \n",
    "        ## Assigning values for each column in table song_info_by_session. \n",
    "        session.execute(query, (line[0], line[9], float(line[5]), int(line[8]), int(line[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithless Music Matters (Mark Knight Dub) 495.30731201171875\n"
     ]
    }
   ],
   "source": [
    "## Selecting data based on the condition given in query-1.\n",
    "query = \"SELECT artist,song,length FROM song_info_by_session \\\n",
    "        WHERE sessionid=338 \\\n",
    "        AND iteminsession=4\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row.artist, row.song, row.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ii) Query-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Query 2: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name)\\\n",
    "## for userid = 10, sessionid = 182\n",
    "table2= \"CREATE TABLE IF NOT EXISTS user_song_info_by_userid\"\n",
    "\n",
    "# Taking user_id and session_id as composite partition keys in order to break the data into chunks so that retrieval \\\n",
    "# process can be facilitated and hotspotting can be avoided.\n",
    "# Taking item_in_session as clustering column in order to sort the data within the partition keys and also \\\n",
    "# to retrive the rows efficiently.\n",
    "query = table2+'(artist text, song text, first_name text, last_name text,\\\n",
    "                 user_id int, session_id int, \\\n",
    "                 item_in_session int,\\\n",
    "                 PRIMARY KEY((user_id,session_id),item_in_session))'\n",
    "try:\n",
    "    session.execute(query)\n",
    "    \n",
    "except Exception as e:\n",
    "    \n",
    "    print(e)        \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file,encoding='utf8') as f:\n",
    "    csv_file = csv.reader(f)\n",
    "    next(csv_file)\n",
    "    for l in csv_file:\n",
    "        ## Assigning the INSERT statements into the `query` variable\n",
    "        query_2 = 'INSERT INTO user_song_info_by_userid (artist, song, first_name, last_name, user_id, session_id, item_in_session)'\n",
    "        ## Assigning values for each column in table song_info_by_session. \n",
    "        query_2 = query_2 + 'VALUES (%s, %s, %s, %s, %s, %s, %s)'\n",
    "        session.execute(query_2, (l[0], l[9], l[1], l[4], int(l[10]), int(l[8]), int(l[3])))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down To The Bone Keep On Keepin' On Sylvie Cruz\n",
      "Three Drives Greece 2000 Sylvie Cruz\n",
      "Sebastien Tellier Kilometer Sylvie Cruz\n",
      "Lonnie Gordon Catch You Baby (Steve Pitron & Max Sanna Radio Edit) Sylvie Cruz\n"
     ]
    }
   ],
   "source": [
    "## Selecting data based on the condition given in query-2.\n",
    "query = \"SELECT artist,song,first_name,last_name, item_in_session FROM user_song_info_by_userid \\\n",
    "         WHERE user_id=10 AND session_id=182\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row.artist,row.song,row.first_name,row.last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# iii) Query-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server: code=2200 [Invalid query] message=\"Index table_3_song_idx_1 is a duplicate of existing index table_3_song_idx\"\n"
     ]
    }
   ],
   "source": [
    "## Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "table3= \"CREATE TABLE IF NOT EXISTS user_info_by_song\"\n",
    "\n",
    "# Taking user_id and song feature as composite partition key in order to divide data in small chunks and facilitate\n",
    "# the retrieval process and also to avoid hotspotting.\n",
    "query = table3+'(first_name text, last_name text,\\\n",
    "                 user_id int, song text, \\\n",
    "                 PRIMARY KEY((song,user_id)))'\n",
    "\n",
    "# Creating an index on column song in table_3 in order to avoid ALLOW FILTERING.\n",
    "index_query = 'CREATE INDEX ON table_3 (song);'\n",
    "\n",
    "try:\n",
    "    session.execute(query)\n",
    "    \n",
    "except Exception as e:\n",
    "    \n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(index_query)\n",
    "    \n",
    "except Exception as e:\n",
    "    \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file,encoding='utf8') as f:\n",
    "    csv_file = csv.reader(f)\n",
    "    next(csv_file)\n",
    "    for l in csv_file:\n",
    "        ## Assigning the INSERT statements into the `query` variable\n",
    "        query_3 = 'INSERT INTO user_info_by_song (first_name, last_name, song, user_id)'\n",
    "        ## Assigning values for each column in table song_info_by_session. \n",
    "        query_3 = query_3 + 'VALUES (%s, %s, %s, %s)'\n",
    "        session.execute(query_3, (l[1], l[4], l[9], int(l[10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sara Johnson\n",
      "Tegan Levine\n",
      "Jacqueline Lynch\n"
     ]
    }
   ],
   "source": [
    "## Selecting data based on the condition given in query-3.\n",
    "query = \"SELECT first_name,last_name FROM user_info_by_song \\\n",
    "         WHERE song='All Hands Against His Own'\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row.first_name,row.last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Queries to drop tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Dropping the table before closing out the sessions\n",
    "drop_table_1 = 'DROP TABLE IF EXISTS song_info_by_session'\n",
    "try:\n",
    "    rows = session.execute(drop_table_1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "drop_table_2 = 'DROP TABLE IF EXISTS user_song_info_by_userid'\n",
    "try:\n",
    "    rows = session.execute(drop_table_2)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "drop_table_3 = 'DROP TABLE IF EXISTS user_info_by_song'\n",
    "try:\n",
    "    session.execute(drop_table_3)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
